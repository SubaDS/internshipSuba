{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec72024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\premium\\anaconda3\\lib\\site-packages (4.11.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from selenium) (0.10.3)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\premium\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\premium\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\premium\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\premium\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\premium\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e185d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries \n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "147b46e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\premium\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\premium\\anaconda3\\lib\\site-packages (from webdriver-manager) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\premium\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.28.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\premium\\anaconda3\\lib\\site-packages (from webdriver-manager) (1.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\premium\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2022.9.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a35b6e",
   "metadata": {},
   "source": [
    "# Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
    "guitars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e8ca5a",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
    "guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a75c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to webdriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08cbf632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching url\n",
    "url_1=(\"https://www.amazon.in/\")\n",
    "driver.get(url_1)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a660bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for the laptops \n",
    "lap=driver.find_element(By.XPATH,\"//input[@type='text']\")\n",
    "lap.send_keys(\"guitar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d163fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click and search for results\n",
    "search_button = driver.find_element(By.XPATH,'//div[@class=\\\"nav-search-submit nav-sprite\\\"]/span/input')    \n",
    "search_button.click()                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444cf2a",
   "metadata": {},
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search\n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then\n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7afa4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape all links for products\n",
    "\n",
    "url_links=[]\n",
    "\n",
    "for x in range(0,3):\n",
    "    link=driver.find_elements(By.XPATH,'//a[@class=\\\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\\\"]')\n",
    "    \n",
    "for i in link:\n",
    "    url_links.append(i.get_attribute(\"href\")\n",
    "                     \n",
    "                     next_page=driver.find_elements(By.XPATH,\"//a[text()='Next']\")\n",
    "                     next_page.click()\n",
    "                     time.sleep(3)\n",
    "                     print(len(url_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c074beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapethe  require data from the pages\n",
    "\n",
    "names=[]\n",
    "brands=[]\n",
    "prices=[]\n",
    "returns=[]\n",
    "expected_delivery=[]\n",
    "availability=[]\n",
    "\n",
    "for i in links:  \n",
    "try:    \n",
    "    driver.get(i)   \n",
    "    try:\n",
    "        name=driver.find_element(By.XPATH,'//h1[@id=\\\"title\\\"]/span')       \n",
    "        names.append(name.text)   \n",
    "        except NoSuchElementException:            \n",
    "            names.append('-')\n",
    "            try:         \n",
    "                brand=driver.find_element(By.XPATH,'//div[@class=\\\"a-section a-spacing-none\\\"]/a')   \n",
    "                brands.append(brand.text)     \n",
    "                except NoSuchElementException:          \n",
    "                    brands.append('-')     \n",
    "                    try:          \n",
    "                        price=driver.find_element(By.XPATH,'//div[@class=\\\"a-section a-spacing-none aok-align-center\\\"]/span[2]')        \n",
    "                        prices.append(price.text.replace('\\\\n','.'))   \n",
    "                        except NoSuchElementException:         \n",
    "                            prices.append('-') \n",
    "                            try:\n",
    "                                ret=driver.find_element(By.XPATH,'//span[@class=\\\"a-declarative\\\"]/div/a')       \n",
    "                                returns.append(ret.text)       \n",
    "                                except NoSuchElementException:          \n",
    "                                    returns.append('-')     \n",
    "                                    try:\n",
    "                                        expected_D=driver.find_element(By.XPATH,'//div[@class=\\\"a-spacing-base\\\"]/span/span[1]')        \n",
    "                                        expected_delivery.append(expected_D.text)     \n",
    "                                        except NoSuchElementException:       \n",
    "                                            expected_delivery.append('-')   \n",
    "                                             try:\n",
    "                                            available=driver.find_element(By.XPATH,'//span[@class=\\\"a-size-medium a-color-success\\\"]')        \n",
    "                                            availability.append(available.text)\n",
    "                                            except NoSuchElementException:          \n",
    "                                                availability.append('-')    \n",
    "                                                except TimeoutError:        \n",
    "                                                    driver.wait(2)      \n",
    "                                                    names.append('-')       \n",
    "                                                    brands.append('-')       \n",
    "                                                    prices.append('-')\n",
    "                                                    returns.append('-')\n",
    "                                                    expected_delivery.append('-')\n",
    "                                                    availability.append('-') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01490449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "df_guitar=pd.DataFrame({ 'name':names,'brand':brands,'price':prices,'returns':returns,'expected_delivery':expected_delivery,'availability':availability})\n",
    "df_guitar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe086558",
   "metadata": {},
   "source": [
    "3.Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f945149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to webdriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24872709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to website\n",
    "driver.get(\"https://images.google.com/\")\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c51edf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting the search icon from site\n",
    "\n",
    "search=driver.find_element(By.XPATH,'//textarea[@class=\"gLFyf\"]')\n",
    "search.send_keys('Images')\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff47a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking search button\n",
    "search_click=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/button/div/span\")\n",
    "search_click.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12e5ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for images with key words fruits cars machine learning guitar and cake\n",
    "\n",
    "keyword=['fruits','cars','Machine Learning','Guitar','Cakes']\n",
    "for i in keyword:\n",
    "    search=driver.find_element(By.XPATH,'//input[@class=\\\"og3lId\\\"]')    \n",
    "    search.clear()   \n",
    "    search.send_keys(str(i))\n",
    "    time.sleep(2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1484a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##dont know how to scrap 10 images each after this for each keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d04d1",
   "metadata": {},
   "source": [
    "4.Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a20f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to webdriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a037336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to website\n",
    "driver.get(\"https://www.flipkart.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8902ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on login_page for close \n",
    "login=driver.find_element(By.XPATH,'//button[@class=\\\"_2KpZ6l _2doB4z\\\"]')\n",
    "login.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eee6afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the value for product\n",
    "\n",
    "Product=driver.find_element(By.XPATH,\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "Product.send_keys(\"oneplus\")\n",
    "\n",
    "# click on search_button\n",
    "\n",
    "search_button=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search_button.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#fetch link for product in first page\n",
    "url_page=[]\n",
    "\n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\\\"_1fQZEK\\\"]')\n",
    "\n",
    "for i in url:\n",
    "    url_page.append(i.get_attribute(\"href\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81e88643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    " print(len(url_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de001a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fetch all require Data\n",
    "brand_names=[]\n",
    "smartphone_name=[]\n",
    "colours=[]\n",
    "rams=[]\n",
    "storages=[]\n",
    "primarys=[]\n",
    "secondarys=[]\n",
    "displays=[]\n",
    "batterys=[]\n",
    "prices=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in url_page: \n",
    "    try:        \n",
    "        driver.get(i)       \n",
    "        Readmore=driver.find_element(By.XPATH,'//button[@class=\\\"_2KpZ6l _1FH0tX\\\"]') \n",
    "        Readmore.click()     \n",
    "        time.sleep(2)      \n",
    "        try:        \n",
    "            brand=driver.find_element(By.XPATH,'//span[@class=\\\"B_NuCI\\\"]')      \n",
    "            brand_names.append(brand.text)       \n",
    "            except NoSuchElementException:         \n",
    "                brand_names.append('-')     \n",
    "                try:           \n",
    "                    smartphone_name=driver.find_element(By.XPATH,'//div[@class=\\\"_3k-BhJ\\\"][1]/table/tbody/tr[3]/td[2]/ul/li')    \n",
    "                    smartphone_names.append(smartphone_name.text)\\n\",\"        \n",
    "                    except NoSuchElementException:\\n\",\"           \n",
    "                        smartphone_names.append('-')\\n\",\"\\n\",\"     \n",
    "                        try:\\n\",\"           \n",
    "                        colour=driver.find_element(By.XPATH,'//div[@class=\\\"_3k-BhJ\\\"][1]/table/tbody/tr[4]/td[2]/ul/li')   \n",
    "                        colours.append(colour.text)\\n\",\"      \n",
    "                    except NoSuchElementException:\n",
    "                    colours.append('-')    \n",
    "                    try:\n",
    "                        ram=driver.find_element(By.XPATH,'//div[@class=\\\"_3k-BhJ\\\"][4]/table/tbody/tr[2]/td[2]/ul/li')    \n",
    "                        rams.append(ram.text)    \n",
    "                            except NoSuchElementException:\n",
    "                                rams.append('-')       \n",
    "                                try:          \n",
    "                                    storage=driver.find_element(By.XPATH,'//div[@class=\\\"_3k-BhJ\\\"][4]/table/tbody/tr[1]/td[2]/ul/li')  \n",
    "                                    storages.append(storage.text)        \n",
    "                                    except NoSuchElementException:            \n",
    "                                        storages.append('-')        \n",
    "                                        try:           \n",
    "                                            primary=driver.find_element(By.XPATH,'//div[@class=\\\"_3k-BhJ\\\"][5]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "                                            primarys.append(primary.text)       \n",
    "                                            except NoSuchElementException:           \n",
    "                                                primarys.append('-')       \n",
    "                                                try:           \n",
    "                                                    secondary=driver.find_element(By.XPATH,'//div[@class=\\\"_3k-BhJ\\\"][5]/table/tbody/tr[6]/td[2]/ul/li')      \n",
    "                                                    secondarys.append(secondary.text)     \n",
    "                                                    except NoSuchElementException:            \n",
    "                                                        secondarys.append('-')    \n",
    "                                                        try:          \n",
    "                                                            display=driver.find_element(By.XPATH,'//div[@class=\\\"_3k-BhJ\\\"][2]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "                                                            displays.append(display.text)    \n",
    "                                                            except NoSuchElementException:          \n",
    "                                                                displays.append('-')        \n",
    "                                                                try:         \n",
    "                                                                    battery=driver.find_element(By.XPATH,'//div[@class=\\\"_2418kt\\\"]/ul/li[4]')       \n",
    "                                                                    batterys.append(battery.text)      \n",
    "                                                                    except NoSuchElementException:            \n",
    "                                                                        batterys.append('-')       \n",
    "                                                                        try:          \n",
    "                                                                            price=driver.find_element(By.XPATH,'//div[@class=\\\"_30jeq3 _16Jk6d\\\"]')  \n",
    "                                                                            prices.append(price.text)       \n",
    "                                                                            except NoSuchElementException:   \n",
    "                                                                                prices.append('-')\n",
    "                                                                                except TimeoutException: \n",
    "                                                                                    \n",
    "                                                                                    time.sleep(1)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DataFrame\n",
    "df_smartphone=pd.DataFrame({'brand_names':brand_namess,'smartphone_name':smartphone_names,'colour':colours,'ram':rams,'storage':storages,'primary':primarys,   \n",
    "                                        'secondary':secondarys,'display':displays,'battery':batterys,'price':prices})\n",
    "df_smartphone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65658f31",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb626643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to webdriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c69467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to google maps url\n",
    "url_map=(\"https://www.google.co.in/maps\")\n",
    "driver.get(url_map)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0224af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching and entering text\n",
    "search = driver.find_element(By.ID,\"searchboxinput\")  \n",
    "\n",
    "search.send_keys(\"chennai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86ddfa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering values in search \n",
    "searchicon = driver.find_element(By.ID,\"searchbox-searchbutton\")   \n",
    "searchicon.click()                     \n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69d6f53c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3654677611.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Premium\\AppData\\Local\\Temp\\ipykernel_7500\\3654677611.py\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    except Exception as e:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "try:  \n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string) \n",
    "    if len(lat_lng):\n",
    "        lat_lng_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            lat = lat_lng_list[0]\n",
    "            lng = lat_lng_list[1]\n",
    "            print(\"Latitude = {}, Longitude = {}\".format(lat, lng))\n",
    "            except Exception as e:\n",
    "                print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af7a77e",
   "metadata": {},
   "source": [
    "6. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "790412a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to webdriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78fedc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://digit.in/\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66d7beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search=driver.find_element(By.XPATH,\"//div[@class='listing_container']//ul//li[9]\")\n",
    "#search.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists\n",
    "Laptop_Name = []\n",
    "Operating_sys = []\n",
    "Display = []\n",
    "Processor = []\n",
    "Memory = []\n",
    "Weight = []\n",
    "Dimensions = []\n",
    "Graph_proc = []\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4afc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the data of laptop names\n",
    "lap_name= driver.find_elements(By.XPATH,'//div[@class=\\\"left_side\\\"]/a/h3')\n",
    "for name in lapt_name:\n",
    "    Laptop_Name.append(name.text)\n",
    "    \n",
    "#Scraping the data of operating system\n",
    "\n",
    "op_sys = driver.find_elements(By.XPATH,\\\"//div[@class='product-detail']/div/ul/li[1]/div/div\\\")  \n",
    "for os in op_sys:\n",
    "                              Operating_sys.append(os.text)\n",
    "                              except NoSuchElementException:\n",
    "                              pass\n",
    "#Scraping data of display of the laptop\n",
    "                              try:   \n",
    "                              display= driver.find_elements(By.XPATH,\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "                              for disp in display:\n",
    "                              Display.append(disp.text)\n",
    "                              except NoSuchElementException:\n",
    "                              pass\n",
    "#Scraping data of Processor\n",
    "                              try:\n",
    "                              processor = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[5]/td[3]\")\n",
    "                              for pro in processor:\n",
    "                              Processor.append(pro.text)\n",
    "                              except NoSuchElementException:\n",
    "                              pass\n",
    "#Scraping data of memory\n",
    "                              try: \n",
    "                              memory = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")\n",
    "                              for memo in memory:\n",
    "                              Memory.append(memo.text)\n",
    "                              except NoSuchElementException:\n",
    "                              pass    \n",
    "#Scraping data of dimensions\n",
    "                              try: \n",
    "                              dimension = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[3]\")\n",
    "                              for dim in dimension:\n",
    "                              Dimensions.append(dim.text)\n",
    "                              except NoSuchElementException:\n",
    "                              pass\n",
    "#Scraping data of Graph processor\n",
    "                              try:\n",
    "                              graph = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[3])\n",
    "                                                           for gra in graph:\n",
    "                                                           Graph_proc.append(gra.text)\n",
    "                                                           except NoSuchElementException:\n",
    "                                                           pass\n",
    "#Scraping data of pric\n",
    "                                                           try:\n",
    "                                                           price = driver.find_elements(By.XPATH,\"//td[@class='smprice']\") \n",
    "                                                           for pri in price:\n",
    "                                                           Price.append(pri.text.replace('â‚¹','Rs '))\n",
    "                                                           except NoSuchElementException:\n",
    "                                                           pass                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb50227e",
   "metadata": {},
   "source": [
    "7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:\n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "195eed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to webdriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9498d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.forbes.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "caa4a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open lists\n",
    "lists = driver.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div[1]/div/div[1]/svg')\n",
    "lists.click()\n",
    "    \n",
    "#Open lists for billionaires\n",
    "\n",
    "billionaires = driver.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div[1]/div/div[2]/ul/li[2]/div[1]/a')\n",
    "billionaires.click()\n",
    "\n",
    "#Open worlds billionaires\n",
    "world_billionaires = driver.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div[1]/div/div[2]/ul/li[2]/div[2]/div[3]/ul/li[1]/a')\n",
    "world_billionaires.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d8d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch require data from datapage\n",
    "rank=[]\n",
    "name=[]\n",
    "net_worth=[]\n",
    "age=[]\n",
    "citizenship=[]\n",
    "source=[]\n",
    "industry=[]\n",
    "for x in range(0,200):   \n",
    "    try:      \n",
    "        ranks = driver.find_elements(By.XPATH,'//div[@class=\\\"rank\\\"]')      \n",
    "        for i in ranks:         \n",
    "            rank.append(i.text)  \n",
    "            names=driver.find_elements(By.XPATH,'//div[@class=\\\"personName\\\"]/div')\n",
    "            for i in names:         \n",
    "                name.append(i.text)      \n",
    "                net_worths=driver.find_elements(By.XPATH,'//div[@class=\\\"netWorth\\\"]/div[1]')\n",
    "                for i in net_worths:\n",
    "                    net_worth.append(i.text)\n",
    "                    ages=driver.find_elements(By.XPATH,'//div[@class=\\\"age\\\"]/div')     \n",
    "                    for i in ages:       \n",
    "                        age.append(i.text)     \n",
    "                        citizenships=driver.find_elements(By.XPATH,'//div[@class=\\\"countryOfCitizenship\\\"]')   \n",
    "                        for i in citizenships:            \n",
    "                            citizenship.append(i.text)     \n",
    "                            sources=driver.find_elements(By.XPATH,'//div[@class=\\\"source\\\"]')    \n",
    "                            for i in sources:        \n",
    "                                source.append(i.text)    \n",
    "                                industrys=driver.find_elements(By.XPATH,'//div[@class=\\\"category\\\"]//div')   \n",
    "                                for i in industrys:         \n",
    "                                    industry.append(i.text)        \n",
    "                                    clicknxt=driver.find_element(By.XPATH,'//button[@class=\\\"pagination-btn pagination-btn--next \\\"]')\\n\",\"   \n",
    "                                    clicknxt.click()     \n",
    "                                    time.sleep(1)\n",
    "                                    except NoSuchElementException:    \n",
    "                                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551d2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating datafram\n",
    "df.bill=pd.DataFrame({'Rank':rank,'Name':name,'Net_worth':net_worth,'Age':age,'Citizenship':citizenship,'Source':source,'Industry':industry})\n",
    "df.bill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d36587",
   "metadata": {},
   "source": [
    "8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "from any YouTube Video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c2893",
   "metadata": {},
   "source": [
    "9.Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall\n",
    "reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f21b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to webdriver\n",
    "#driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.hostelworld.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e6e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the location\n",
    "loc = driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[2]/div/div/div[4]/div/div[2]/div/div[1]/div/div/div/input')\n",
    "loc.send_keys(\"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f14542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting first option\n",
    "opt=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[2]/div/div/div[4]/div/div[2]/div/div[1]/div/div/ul/li[2]/div')\n",
    "opt.click()\n",
    "\n",
    "#click on lets go button\n",
    "go=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[2]/div/div/div[4]/div/div[2]/div/div[5]/button')\n",
    "go.click()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef55873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"# Scrapping required details,\"\n",
    "#Creating Empty Lists\n",
    "Name = []\n",
    "Distance = []\n",
    "Rating = []\n",
    "Reviews = []\n",
    "Overall = []\n",
    "Privates = []\n",
    "Dorms = []\n",
    "Facilities = []\n",
    "Property = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dfd56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping Names\n",
    "Na= driver.find_elements(By.XPATH,'//h2[@class=\\\"title title-6\\\"]')\n",
    "for i in Na\n",
    "    Name.append(i.text)\n",
    "\n",
    "#Scrapping distance from city centre\n",
    "    try:\n",
    "        Dis = driver.find_elements(By.XPATH,'//span[@class=\\\"description\\\"]')\n",
    "        for i in Dis: \n",
    "            Distance.append(i.text)\n",
    "            except NoSuchElementException:  \n",
    "                Distance.append(\\\"-\\\")\n",
    "                               \n",
    "#Scrapping ratings\n",
    "                                try: \n",
    "                                Rat = driver.find_elements(By.XPATH,'//div[@class=\\\"rating rating-summary-container big\\\"]')  \n",
    "                                for i in Rat:  \n",
    "                                Rating.append(i.text[:][0:3])\n",
    "                                except NoSuchElementException:  \n",
    "                                Rating.append(\\\"-\\\") \n",
    "#Scrapping total reviews\n",
    "                                              try:\n",
    "                                              Rev= driver.find_elements(By.XPATH,'//div[@class=\\\"reviews\\\"]')\n",
    "                                              for i in Rev:     \n",
    "                                              Reviews.append(i.text)\n",
    "                                              except NoSuchElementException: \n",
    "                                              Reviews.append(\\\"-\\\") \n",
    "\n",
    "#Scrapping overall reviews\n",
    "                                                             try:\n",
    "                                                             Ove = driver.find_elements(By.XPATH,'//div[@class=\\\"keyword\\\"]')\n",
    "                                                             for i in Ove: \n",
    "                                                             Overall.append(i.text)\n",
    "                                                             except NoSuchElementException:\n",
    "                                                             Overall.append(\\\"-\\\")                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cant scrap prices ,dorms properly\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
